#!/usr/bin/env python
import argparse
import datetime
import os
import subprocess
import psutil
import sys
import threading
import time
from itertools import combinations
import copy
import random
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
import pandas as pd

max_memory_usage = 0
def monitor_memory_usage(interval=1):
    global max_memory_usage
    while True:
        total_memory = get_total_memory_usage()
        max_memory_usage = max(max_memory_usage, total_memory)
        time.sleep(interval)

def get_total_memory_usage():
    parent_process = psutil.Process(os.getpid())
    parent_memory = parent_process.memory_info().rss
    total_memory = parent_memory

    for child in parent_process.children(recursive=True):
        try:
            child_memory = child.memory_info().rss
            total_memory += child_memory
        except psutil.NoSuchProcess:
            continue
    return total_memory

# Start the memory monitoring in a separate thread
monitoring_thread = threading.Thread(target=monitor_memory_usage, args=(1,), daemon=True)
monitoring_thread.start()

def logging(message):
    print(f"[{datetime.datetime.now()}] {message}", flush=True)

def run_cmd(command, name, verbose=True):
    if verbose:
        logging(f'{name}')
    if "-o" in command:
        index = command.index("-o")
        out = command[index + 1]
        log_file = out + ".log"
        with open(log_file, 'w') as f:
            ret = subprocess.run(command, stdout=f)
    else:
        ret = subprocess.run(command)
    if ret.returncode:
        logging(f'{name} fails: {" ".join(command)}')
        sys.exit(1)

def add_random_prefix_suffix(input_file, output_file, prefix_length=2000, suffix_length=2000, seed=None):
    if seed is None:
        seed = random.randint(0, 2**32 - 1)
    random.seed(seed)
    
    # Define the possible bases
    bases = ['A', 'T', 'C', 'G']
    
    def random_sequence(length):
        return ''.join(random.choices(bases, k=length))
    
    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
        for record in SeqIO.parse(infile, "fasta"):
            prefix = random_sequence(prefix_length)
            suffix = random_sequence(suffix_length)
            new_seq = Seq(prefix + str(record.seq) + suffix)
            new_record = SeqRecord(new_seq, id=record.id, description=record.description)
            SeqIO.write(new_record, outfile, "fasta")
    
def adjust_coordinates(blocks):
    df = pd.read_csv(blocks, sep = '\t')
    df["End Pos in Original Sequence"] -= 2000
    df["Start Pos in Original Sequence"] -= 2000

    df.loc[df["End Pos in Original Sequence"]<=0, "End Pos in Original Sequence"] = 0
    df.loc[df["Start Pos in Original Sequence"]<=0, "Start Pos in Original Sequence"] = 0

    df.iloc[len(df)-1, 9] -= 2000
    df["Length in Original Sequence"] = df["End Pos in Original Sequence"] - df["Start Pos in Original Sequence"]
    df = df.loc[df["Length in Original Sequence"] >= 2000]
    df.to_csv(blocks, index=None, sep='\t')

def get_coordinates(edlib, genome_ori, genome, blocks, output):
    run_cmd(["edlib", genome_ori, genome, edlib], f"Aligning transformed sequence {genome} to original sequence {genome_ori}")
    run_cmd(["parse_cigar.py", f"{edlib}/cigar.txt", blocks, "edlib"], f"Finalizing genome {genome_ori}")
    run_cmd(["mv", f"{blocks}.edlib.csv", output], f"Copying result {output}")
    adjust_coordinates(output)

parser = argparse.ArgumentParser()
parser.add_argument(
    "-g",
    "--genome",
    required=True,
    action="append",
    help="Path to the genome"
)
parser.add_argument(
    "-i",
    "--iterations",
    required=False,
    type=int,
    default=5,
    help="The number of iterations for three genomes only"
)
parser.add_argument(
    "-k",
    "--k_values",
    required=False,
    default="31,51,101,201,401,801,2001",
    help="K-mer values for graph transformations (default 31,51,101,201,401,801,2001)"
)
parser.add_argument(
    "-s",
    "--simple",
    required=False,
    default="0",
    type=str,
    help="Similarity threshold for collapsing simple bulges (default 0)"
)
parser.add_argument(
    "-c",
    "--complex",
    required=False,
    default="0.65",
    type=str,
    help="Similarity threshold for collapsing complex bulges (default 0.65)"
)
parser.add_argument(
    "-o",
    "--output",
    required=True,
    help="Output directory of GenomeDecoder"
)

args = parser.parse_args()

if (os.path.isdir(args.output)):
    logging(f"Output directory {args.output} already exists.")
    exit(1)

k_values = args.k_values.split(',')
if len(k_values) < 2:
    logging(f"K should be more than 2 values.")

pid = os.getpid()
process = psutil.Process(pid)
logging("GenomeDecoder starts")
logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")
# create output directory
run_cmd(["mkdir", args.output], "Making output dir")

genomes = args.genome
genomes_to_run = []
for i, g in enumerate(genomes):
    add_random_prefix_suffix(g, f"{args.output}/original_genome_{i+1}.pre_suff.fa")
    genomes_to_run.append(f"{args.output}/original_genome_{i+1}.pre_suff.fa")

if len(genomes_to_run) == 1:
    genome1_ori = os.path.realpath(genomes_to_run[0])
    genome1 = os.path.realpath(genomes_to_run[0])
    k = k_values[0]
    out_dir_graph = f"{args.output}/jumbodbg.consensus.k{k}"
    dot_graph = f"{out_dir_graph}/graph.dot"
    fa_graph = f"{out_dir_graph}/graph.fasta"
    out_dir_consensus = f"{args.output}/consensus.k{k}"
    run_cmd(["jumboDBG", "--reads", genome1, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
    run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-k", k, "-o", out_dir_consensus, "--only_simple_bulge", "-s", "0.9"], f"Graph transformation for k={k}")
    genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"

    for k in k_values[1:]:
        out_dir_graph = f"{args.output}/jumbodbg.consensus.k{k}"
        dot_graph = f"{out_dir_graph}/graph.dot"
        fa_graph = f"{out_dir_graph}/graph.fasta"
        out_dir_consensus = f"{args.output}/consensus.k{k}"
        run_cmd(["jumboDBG", "--reads", genome1, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
        run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
        genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"

    out = f"{k}.1"
    out_dir_graph = f"{args.output}/jumbodbg.consensus.k{out}"
    dot_graph = f"{out_dir_graph}/graph.dot"
    fa_graph = f"{out_dir_graph}/graph.fasta"
    out_dir_consensus = f"{args.output}/consensus.k{out}"
    run_cmd(["jumboDBG", "--reads", genome1, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
    run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
    genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

    edlib_1 = f"{out_dir_consensus}.edlib1"
    run_cmd(["edlib", genome1_ori, genome1, edlib_1], "Aligning transformed sequence 1 to original sequence 1")
    run_cmd(["parse_cigar.py", f"{edlib_1}/cigar.txt", f"{out_dir_consensus}/graph.with_haplome.haplome1.blocks", "edlib"], "Finalizing genome 1")
    run_cmd(["mv", f"{out_dir_consensus}/graph.with_haplome.haplome1.blocks.edlib.csv", f"{args.output}/final_blocks_1.csv"], "Copying result 1")
    adjust_coordinates(f"{args.output}/final_blocks_1.csv")
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

elif len(genomes_to_run) == 2:
    genome1_ori = os.path.realpath(genomes_to_run[0])
    genome2_ori = os.path.realpath(genomes_to_run[1])
    genome1 = os.path.realpath(genomes_to_run[0])
    genome2 = os.path.realpath(genomes_to_run[1])

    k = k_values[0]
    out_dir_graph = f"{args.output}/jumbodbg.consensus.k{k}"
    dot_graph = f"{out_dir_graph}/graph.dot"
    fa_graph = f"{out_dir_graph}/graph.fasta"
    out_dir_consensus = f"{args.output}/consensus.k{k}"
    run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
    run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "--only_simple_bulge", "-s", "0.9"], f"Graph transformation for k={k}")
    genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
    genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"

    for k in k_values[1:]:
        out_dir_graph = f"{args.output}/jumbodbg.consensus.k{k}"
        dot_graph = f"{out_dir_graph}/graph.dot"
        fa_graph = f"{out_dir_graph}/graph.fasta"
        out_dir_consensus = f"{args.output}/consensus.k{k}"
        run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
        run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
        genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
        genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"

    out = f"{k}.1"
    out_dir_graph = f"{args.output}/jumbodbg.consensus.k{out}"
    dot_graph = f"{out_dir_graph}/graph.dot"
    fa_graph = f"{out_dir_graph}/graph.fasta"
    out_dir_consensus = f"{args.output}/consensus.k{out}"
    run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
    run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
    genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
    genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

    edlib_1 = f"{out_dir_consensus}.edlib1"
    blocks_1 = f"{out_dir_consensus}/graph.with_haplome.haplome1.blocks"
    output_1 = f"{args.output}/final_blocks_1.csv"
    align_thread_1 = threading.Thread(target=get_coordinates, args=(edlib_1, genome1_ori, genome1, blocks_1, output_1,))
    align_thread_1.start()
    
    edlib_2 = f"{out_dir_consensus}.edlib2"
    blocks_2 = f"{out_dir_consensus}/graph.with_haplome.haplome2.blocks"
    output_2 = f"{args.output}/final_blocks_2.csv"
    align_thread_2 = threading.Thread(target=get_coordinates, args=(edlib_2, genome2_ori, genome2, blocks_2, output_2,))
    align_thread_2.start()

    align_thread_1.join()
    align_thread_2.join()
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

else:
    # how many iterations of full combinations
    genomes_ori = copy.deepcopy(genomes_to_run)
    for it in range(args.iterations):
        for index1, index2 in combinations(range(len(genomes_to_run)), 2):
            genome1 = os.path.realpath(genomes_to_run[index1])
            genome2 = os.path.realpath(genomes_to_run[index2])
            logging(f"Run synteny for {genome1} and {genome2}")
            
            output = f"{args.output}/{it}_genome{index1+1}_genome{index2+1}"
            run_cmd(["mkdir", output], "Making output dir")
            k = k_values[0]
            out_dir_graph = f"{output}/jumbodbg.consensus.k{k}"
            dot_graph = f"{out_dir_graph}/graph.dot"
            fa_graph = f"{out_dir_graph}/graph.fasta"
            out_dir_consensus = f"{output}/consensus.k{k}"
            run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
            run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "--only_simple_bulge", "-s", "0.9"], f"Graph transformation for k={k}")
            genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
            genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"

            for k in k_values[1:]:
                out_dir_graph = f"{output}/jumbodbg.consensus.k{k}"
                dot_graph = f"{out_dir_graph}/graph.dot"
                fa_graph = f"{out_dir_graph}/graph.fasta"
                out_dir_consensus = f"{output}/consensus.k{k}"
                run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
                run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
                genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
                genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"

            out = f"{k}.1"
            out_dir_graph = f"{output}/jumbodbg.consensus.k{out}"
            dot_graph = f"{out_dir_graph}/graph.dot"
            fa_graph = f"{out_dir_graph}/graph.fasta"
            out_dir_consensus = f"{output}/consensus.k{out}"
            run_cmd(["jumboDBG", "--reads", genome1, "--reads", genome2, "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"], f"Graph construction for k={k}")
            run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genome1, "-2", genome2, "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex], f"Graph transformation for k={k}")
            genome1 = f"{out_dir_consensus}/graph.final.haplome1.fa"
            genome2 = f"{out_dir_consensus}/graph.final.haplome2.fa"
            logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

            genomes_to_run[index1] = genome1
            genomes_to_run[index2] = genome2
    
    # jumboDBG
    out_dir_graph = f"{args.output}/jumbodbg_final"
    print("Run cmd:", " ".join(["jumboDBG", "--reads", " --reads ".join(genomes_to_run), "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"]))
    os.system(" ".join(["jumboDBG", "--reads", " --reads ".join(genomes_to_run), "-t", "25", "-k", k, "-o", out_dir_graph, "--coverage"]))
    # consensus (fixed for 3 genomes)
    dot_graph = f"{out_dir_graph}/graph.dot"
    fa_graph = f"{out_dir_graph}/graph.fasta"
    out_dir_consensus = f"{args.output}/consensus_final"
    run_cmd(["consensus_asm", "-d", dot_graph, "-f", fa_graph, "-1", genomes_to_run[0], "-2", genomes_to_run[1], "-3", genomes_to_run[2], "-k", k, "-o", out_dir_consensus, "-s", args.simple, "-c", args.complex, "--only_synteny"], f"Graph transformation for k={k}")
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

    threads = []
    for i in range(len(genomes_to_run)):
        edlib = f"{out_dir_consensus}.edlib{i+1}"
        blocks = f"{out_dir_consensus}/graph.with_haplome.haplome{i+1}.blocks"
        output = f"{args.output}/final_blocks_{i+1}.csv"
        align_thread = threading.Thread(target=get_coordinates, args=(edlib, genomes_ori[i],  genomes_to_run[i], blocks, output,))
        threads.append(align_thread)
    for t in threads:
        t.join()
    logging(f"RSS (Resident Set Size): {max_memory_usage} bytes")

logging("GenomeDecoder finished")